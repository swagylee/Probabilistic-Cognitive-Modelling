{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "University of Helsinki, Master's Programme in Data Science  \n",
    "DATA20047 Probabilistic Cognitive Modelling - Spring 2023  \n",
    "Luigi Acerbi  \n",
    "\n",
    "# Problem Set 4: Combining inference with utility, and everything\n",
    "\n",
    "- This homework problem set focuses on **Week 7** of the course, plus a recap of everything we have done in the course.\n",
    "- This problem set is worth **30 points** in total (out of 100 for the full course).\n",
    "- Check the submission deadline on Moodle! **Note that the deadline is at noon.**\n",
    "\n",
    "\n",
    "## Submission instructions\n",
    "\n",
    "Submission must be perfomed entirely on Moodle (**not** by email).\n",
    "1. When you have completed the exercises, save the notebook.\n",
    "2. Report your solutions and answers on Moodle (\"*Problem set 4 answer return*\").\n",
    "3. Submit two files on Moodle (\"*Problem set 4 notebook return*\"): \n",
    "  - The notebook as `.ipynb`.\n",
    "  - The same notebook downloaded as `.pdf` (there are various ways to save the file as PDF, the most general is \"File\" > \"Print Preview\" and then print the page to PDF using your browser - remember to enter the Print Preview first).\n",
    "\n",
    "## IMPORTANT\n",
    "\n",
    "1. Do not share your code and answers with others. Contrary to the class exercises, which you can do with others, these problems are *not* group work and must be done individually.\n",
    "2. It is allowed to use snippets of code from the lecture exercises and model solutions.\n",
    "3. It is your responsibility to ensure that the notebook has fully finished running all the cells, all the plots view properly etc. before submitting it. However, the notebook should be runnable from scratch if needed (\"Kernel > Restart & Run All\").\n",
    "4. Submit your work by the deadline.\n",
    "5. Unless stated otherwise, please report your numerical answers in Moodle with full numerical precision (~14-15 digits), unless the answer is an integer.\n",
    "6. If you are confused, think there is a mistake or find things too difficult, please ask on Moodle.\n",
    "\n",
    "## References\n",
    "\n",
    "- \\[**MKG22**\\] Ma WJ, KÃ¶rding K, and Goldreich D. \"Bayesian Models of Perception and Action: An Introduction\". MIT Press, 2022.\n",
    "- \\[**AWV12**\\] Acerbi L, Wolpert DM, Vijayakumar S. \"Internal Representations of Temporal Statistics and Feedback Calibrate Motor-Sensory Interval Timing\". *PLoS Computational Biology*, 2012. [Link](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002771)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up -- do not change\n",
    "#789 10 11 12\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import scipy as sp\n",
    "import scipy.stats as sps\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "npr.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4.1 (6 pts)\n",
    "\n",
    "> In this question, we will look at the optimal aim location when playing a modified game of darts.\n",
    "\n",
    "Assume you are playing a game of darts, that is you need to throw a dart at a target a couple of meters away from you. For simplicity, we assume this is a modified version of the game in which the target board comprises of a series of vertical bands, so that only the horizontal landing location of the dart matters. Landing the dart on each different sector is associated with a different score.\n",
    "\n",
    "We measure horizontal location starting from the center of the dart board ($0$ cm), with negative numbers meaning a position to the left of the center and positive numbers a position to the right of the center. We ignore the vertical position, assuming the bands are tall enough to be easy to hit on the vertical axis.\n",
    "\n",
    "The sectors are characterized by their bounds on the board (start and end location) and their score, as follows:\n",
    "\n",
    "- -25 to -15 cm: 15 points.\n",
    "- -15 to -5 cm: 10 points.\n",
    "- -5 to 5 cm: 20 points.\n",
    "- 5 to 15 cm: 5 points.\n",
    "- 15 to 25 cm: 25 points.\n",
    "\n",
    "If the dart lands outside the board, you would get 0 points.\n",
    "\n",
    "Given the aim location $\\hat{s}$, we assume that due to motor noise, the actual *hit* location $r$ (where the dart actually lands) is distributed as follows:\n",
    "$$\n",
    "p(r|\\hat{s}) = (1-\\lambda) \\mathcal{N}\\left(r; \\hat{s},\\sigma^2_\\text{motor}\\right) + \\lambda \\mathcal{N}\\left(r; \\hat{s},\\sigma^2_\\text{lapse}\\right),\n",
    "$$\n",
    "where $\\sigma_\\text{motor}$ is the standard spread location due to motor error and throwing variability. The novelty of this equation is that we also consider a probability $\\lambda \\in [0, 1]$ of lapsing (e.g., sneezing or being distracted by a friend while throwing), which produces a bad throw with a larger error $\\sigma_\\text{lapse}$. For this exercise, we assume $\\sigma_\\text{lapse} = 20$ cm.\n",
    "\n",
    "Write a function that computes the expected score as a function of aim location $\\hat{s}$ and task parameters $\\left(\\sigma_\\text{motor}, \\lambda, \\sigma_\\text{lapse}\\right)$. Then, write a function that for given task parameters $\\left(\\sigma_\\text{motor}, \\lambda, \\sigma_\\text{lapse}\\right)$, returns the optimal aim location $s^\\star$ (the aim point which produces the maximum expected score).\n",
    "\n",
    "- a) For $\\sigma_\\text{motor} \\in [2, 10]$ cm and $\\lambda \\in [0, 0.2]$, what are the rightmost (maximum) and leftmost (minimum) locations for $s^\\star$?\n",
    "- b) For $\\lambda = 0.1$, plot $s^\\star$ as a function of $\\sigma_\\text{motor}$, for $\\sigma_\\text{motor} \\in [2, 10]$ cm. You should see an interesting switch of optimal strategy. Around which value of $\\sigma_\\text{motor}$ the optimal strategy switches and it becomes more convenient to throw the dart to the left of the center as opposed to the right of the center?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bands = [-np.inf,-25,-15,-5,5,15,25,np.inf]\n",
    "scores = [0,15,10,20,5,25,0]\n",
    "\n",
    "# Plot target score\n",
    "\n",
    "s = np.linspace(-30,30,200)\n",
    "for index, band in enumerate(scores):\n",
    "    a = np.maximum(bands[index], -40)\n",
    "    b = np.minimum(bands[index+1], 40)\n",
    "    plt.plot((a,a,b,b),np.array((0,scores[index],scores[index],0)))\n",
    "\n",
    "plt.xlabel('r (cm)')\n",
    "plt.ylabel('score (points)')\n",
    "plt.title('Target score based on landing point')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "def get_score(hit):\n",
    "    scores = []\n",
    "    for i in range(len(hit)):\n",
    "        if -25 <=hit[i] and hit[i]< -15:\n",
    "            scores.append(15)\n",
    "            continue\n",
    "        if -15 <=hit[i] and hit[i]< -5:\n",
    "            scores.append(10)\n",
    "            continue\n",
    "        if -5 <= hit[i] and hit[i]< 5:\n",
    "            scores.append(20)\n",
    "            continue\n",
    "        if 5 <=hit[i] and hit[i]< 15:\n",
    "            scores.append(5)\n",
    "            continue\n",
    "        if 15 <=hit[i] and hit[i]<= 25:\n",
    "            scores.append(25)\n",
    "            continue\n",
    "        if 25 < hit[i] or hit[i]< -25:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "    return scores\n",
    "def expected_score(s_hat, sigma_motor, lmbda, sigma_lapse):\n",
    "    hit = np.linspace(-41, 41, 2**8+1)\n",
    "    dr = hit[1] - hit[0]\n",
    "    scores = get_score(hit)\n",
    "    p_hit = (1 - lmbda) * norm.pdf(hit, loc=s_hat, scale=sigma_motor) + lmbda * norm.pdf(hit, loc=s_hat, scale=sigma_lapse)\n",
    "    const = sp.integrate.romb(p_hit, dx=dr, axis=0)\n",
    "    posterior = p_hit / const\n",
    "    expected = sp.integrate.romb(posterior * scores, axis=0)\n",
    "    return expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_aim_location(sigma_motor, lambd, sigma_lapse):\n",
    "    results = []\n",
    "    for s_hat in np.linspace(-25, 25, 200):\n",
    "        score = expected_score(s_hat, sigma_motor, lambd, sigma_lapse)\n",
    "        results.append(score)\n",
    "    print(results)\n",
    "    s_star = np.linspace(-25, 25, 200)[np.argmax(results)]\n",
    "    return s_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_motor_range = np.linspace(2, 10, 80)\n",
    "lmbda_range = np.linspace(0, 0.2, 20)\n",
    "optimal_locations = np.zeros((len(sigma_motor_range), len(lmbda_range)))\n",
    "for i, sigma_motor in enumerate(sigma_motor_range):\n",
    "    for j, lmbda in enumerate(lmbda_range):\n",
    "        optimal_locations[i, j] = find_optimal_aim_location(sigma_motor, lmbda, 20)\n",
    "rightmost = np.max(optimal_locations)\n",
    "leftmost = np.min(optimal_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rightmost optimal aim location: 19.9748743719 cm\n",
      "Leftmost optimal aim location: -1.8844221106 cm\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rightmost optimal aim location: {rightmost:.10f} cm\")\n",
    "print(f\"Leftmost optimal aim location: {leftmost:.10f} cm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_star = []\n",
    "for sigma_motor in sigma_motor_range:\n",
    "    s_star.append(find_optimal_aim_location(sigma_motor, 0.1,20))\n",
    "plt.plot(sigma_motor_range, s_star)\n",
    "plt.xlabel('sigma_motor')\n",
    "plt.ylabel('optimal aim location (cm)')\n",
    "plt.title('Optimal aim location as a function of sigma_motor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.959798994974868"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_star[58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.974683544303797"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma_motor_range[59]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4.2 (6 pts)\n",
    "\n",
    "> In this question, we will infer the empirical loss function of an observer in a task based on their responses.\n",
    "\n",
    "In each trial of this experiment, the participant is shown a cloud of dots on a screen. The horizontal location of each dot is drawn from the following mixture distribution:\n",
    "  $$\n",
    "  p(s; h) = 0.8 \\mathcal{N}\\left(s; 0, 0.2^2\\right) + 0.2 \\mathcal{N}\\left(s; h, 0.2^2\\right)\n",
    "  $$\n",
    "  where $h$ is a parameter controlled by the experimenter, and is changed from trial to trial. The horizontal location is measured in normalized screen units $s \\in [-1, 1]$, where $0$ is the center of the screen. \n",
    "- Along the vertical axis, the dots show a small jitter along the center of the screen (we ignore the vertical displacement).\n",
    "- In each trial, the subject is asked to report the horizontal position of the *center* of the cloud of dots. The word \"center\" is ambiguous, as we could be asking for the mean, the median, or other [measures of central tendency](https://en.wikipedia.org/wiki/Central_tendency). The question is what the subject would naturally report. We assume the subject reports the estimate $\\hat{s}$ that minimizes their expected loss. \n",
    "- For the purpose of this question, we also assume that the observer's loss function takes the parametric form\n",
    "  $$\n",
    "  \\mathcal{L}_\\alpha\\left(s^\\prime, s \\right) = \\left| s^\\prime - s \\right|^\\alpha\n",
    "  $$\n",
    "  which is the loss for reporting $s^\\prime$ for a dot position $s$. \n",
    "- In each trial, the expected loss is the expectation of the loss over the presented dot locations, $p(s; h)$.\n",
    "- A sequence of trials is thus represented by the trial design (the presented distribution of dots, parameterized by $h_t$) and the observer's responses $\\hat{s}_t$ in each trial $t$.\n",
    "- For simplicity, here we assume that there is no response noise.\n",
    "\n",
    "Given the data reported below, infer the exponent $\\alpha \\ge 0$ of the loss function that best describes the subject behavior. Do so by finding the $\\alpha$ that minimizes the squared error between the model prediction (assuming the observer follows Bayesian decision theory) and the data. Report the best estimate for $\\alpha$ in Moodle (error tolerance $\\pm 0.1$).\n",
    "\n",
    "*Notes*: \n",
    "- Assume that the horizontal location $s \\in [-1, 1]$ (i.e., within the width of the screen).\n",
    "- In real data, the observer's responses would be corrupted by decision and motor noise, which would make inferring the loss function still more complex. For example, it would be better to use Bayesian inference to infer a posterior over $\\alpha$, as opposed to a point estimate. Still, the logic would be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial parameters h_t and subject's responses s_hat_t\n",
    "\n",
    "h_t = np.array([-0.0663824 ,  0.17625959, -0.3999085 , -0.15813394, -0.28259529,\n",
    "       -0.32612912, -0.25099183, -0.12355142, -0.08258602,  0.03105339,\n",
    "       -0.06464439,  0.1481756 , -0.2364382 ,  0.30249395, -0.37808993,\n",
    "        0.13637401, -0.06615616,  0.04695186, -0.28769045, -0.24151881])\n",
    "s_hat_t = np.array([-0.0125   ,  0.034375 , -0.0734375, -0.03125  , -0.0546875,\n",
    "       -0.0609375, -0.0484375, -0.025    , -0.0171875,  0.00625  ,\n",
    "       -0.0125   ,  0.0296875, -0.0453125,  0.0578125, -0.0703125,\n",
    "        0.0265625, -0.0125   ,  0.009375 , -0.0546875, -0.046875 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_real = np.linspace(-1,1,2**6+1)\n",
    "s_dot = np.linspace(-1,1,2**6+1)\n",
    "loss = np.zeros((2**6+1,2**6+1))\n",
    "pdf = np.zeros((2**6+1,2**6+1))\n",
    "e_loss = np.zeros((2**6+1,1))\n",
    "for i in range(2**6+1):\n",
    "    for j in range(2**6+1):\n",
    "        loss[i,j] = np.sum(np.abs(s_dot[i] - s_real[j])**0.1)\n",
    "ds_real = s_real[1] - s_real[0]\n",
    "s_real_pdf = 0.8 * sps.norm.pdf(s_real,loc = 0, scale = 0.2) + 0.2 * sps.norm.pdf(s_real,loc = h,scale = 0.2)\n",
    "norm = sp.integrate.romb(s_real_pdf, dx = ds_real, axis=0)\n",
    "s_real_pdf = s_real_pdf / norm\n",
    "for i in range(2**6+1):\n",
    "    pdf[i,:] = s_real_pdf * loss[i,:]\n",
    "ex_loss = sp.integrate.romb(pdf, dx = ds_real, axis=0)\n",
    "ex_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def expected_loss(h, alpha):\n",
    "    s_real = np.linspace(-1,1,2**6+1)\n",
    "    s_dot = np.linspace(-1,1,2**6+1)\n",
    "    loss = np.zeros((2**6+1,2**6+1))\n",
    "    pdf = np.zeros((2**6+1,2**6+1))\n",
    "    for i in range(2**6+1):\n",
    "        for j in range(2**6+1):\n",
    "            loss[i,j] = np.sum(np.abs(s_dot[i] - s_real[j])**alpha)\n",
    "    ds_real = s_real[1] - s_real[0]\n",
    "    s_real_pdf = 0.8 * sps.norm.pdf(s_real,loc = 0, scale = 0.2) + 0.2 * sps.norm.pdf(s_real,loc = h,scale = 0.2)\n",
    "    norm = sp.integrate.romb(s_real_pdf, dx = ds_real, axis=0)\n",
    "    s_real_pdf = s_real_pdf / norm\n",
    "    for i in range(2**6+1):\n",
    "        pdf[i,:] = s_real_pdf * loss[i,:]\n",
    "    ex_loss = sp.integrate.romb(pdf, dx = ds_real, axis=1)\n",
    "    return ex_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6080402010050252"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = np.linspace(0, 20, 200)\n",
    "predictions = np.zeros((20,20))\n",
    "error = np.zeros_like(alphas)\n",
    "s = np.linspace(-1,1,2**6+1)\n",
    "for i, alpha in enumerate(alphas):\n",
    "    prediction = []\n",
    "    for h, s_hat in zip(h_t, s_hat_t):\n",
    "        s_pred = s[np.argmin(expected_loss(h, alpha))]\n",
    "        prediction.append(s_pred)\n",
    "    error[i] = np.sum((np.array(prediction) - s_hat_t) ** 2)\n",
    "a_star = alphas[np.argmin(error)]\n",
    "a_star\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4.3 (18 pts)\n",
    "\n",
    "> For this question, which amounts to a small modelling and model fitting project, we will put together several topics seen in the course. Using the data of the sensorimotor timing experiment \\[**AWV12**\\] that we encountered several times in this course, we will use a Bayesian observer model to recover the observer's prior from their responses.\n",
    "\n",
    "We analyze the data with the `gaussianmixobserverwithlapse` model, defines as follows:\n",
    "\n",
    "- We assume the observer builds a (mismatched) Gaussian prior with two components:\n",
    "  $$p(s) = w_\\text{prior} \\mathcal{N}\\left(s| \\mu_{\\text{prior}}, \\sigma_{\\text{prior}, 1}^2 \\right) + \n",
    "  \\left(1 - w_\\text{prior} \\right)\\mathcal{N}\\left(s| \\mu_{\\text{prior}}, \\sigma_{\\text{prior}, 2}^2 \\right)$$ \n",
    "  over the stimuli (time intervals). We assume the two components have the same mean $\\mu_\\text{prior}$, but different weights ($w_\\text{prior}$ and $1 - w_\\text{prior}$) and standard deviations ($\\sigma_{\\text{prior}, 1}$ and $\\sigma_{\\text{prior}, 2}$).\n",
    "- We assume that the measurement distribution and likelihood are also Gaussian, $p(x|s) = \\mathcal{N}\\left(x| s, \\sigma^2 \\right)$.\n",
    "- The observer uses the *posterior mean* estimator for the value of the stimulus, $\\hat{s}_\\text{PM}$.\n",
    "- Gaussian motor response noise is added to the estimate, $p(r|\\hat{s}) = \\mathcal{N}\\left(r| \\hat{s}, \\sigma_\\text{motor}^2 \\right)$. For this exercise, we assume that $\\sigma_\\text{motor} = 70$ ms for all subjects.\n",
    "- In each trial, the observer lapses with probability $\\lambda$ (the *lapse rate*), in which case the response is drawn from $p_\\text{lapse}(r) = \\text{Uniform}\\left(r; 0, 1500 \\right)$ ms. Otherwise, the observer responds normally (according to $p(r|\\hat{s})$ described above) with probability $1 - \\lambda$. \n",
    "- The free parameters of this model are $\\mathbf{\\theta} = \\left(\\sigma, \\lambda, w_\\text{prior}, \\mu_\\text{prior}, \\sigma_{\\text{prior}, 1}, \\sigma_{\\text{prior}, 2} \\right)$.\n",
    "\n",
    "Note that the parameters $w_\\text{prior}, \\mu_\\text{prior}, \\sigma_{\\text{prior}, 1}, \\sigma_{\\text{prior,2}}$ model the subject's prior. In other words, we can use the estimates of these parameters to visualize what the subject's prior might look like.\n",
    "\n",
    "-----------------------------------------------\n",
    "\n",
    "For this analysis, we will separately consider all subjects, but for all subjects' datasets we will discard the first session, to ensure that subjects have achieved enough training in the task. We provide below example code that retrieves the stimuli $\\mathbf{s}$ and responses $\\mathbf{r}$ for a subject, withouth the first session.\n",
    "\n",
    "- a) As a sanity check, compute the log-likelihood of model parameter $\\theta_\\star = \\left(\\sigma = 100, \\lambda = 0.01, w_\\text{prior} = 0.5, \\mu_\\text{prior} = 787, \\sigma_{\\text{prior}, 1} = 100, \\sigma_{\\text{prior}, 2} = 120 \\right)$ for the dataset of subject 5 (having removed the first session) and report the result in Moodle.\n",
    "- b) Separately fit the `gaussianmixobserverwithlapse` model to all the six subjects' datasets (removing the first session from all data) via maximum-likelihood estimation. For each subject, report the maximum log-likelihood value in Moodle.\n",
    "- c) Consider now the `idealgaussianobserverwithlapse` observer model. The `idealgaussianobserverwithlapse` is the same as the model above with the difference that the observer's prior is a single Gaussian $p(s) = \\mathcal{N}\\left(s | \\mu_\\text{prior}, \\sigma^2_\\text{prior} \\right)$ with $\\mu_\\text{prior} = 787.5$ ms and $\\sigma_{\\text{prior}} = 128.1$ ms. As above, we fix $\\sigma_\\text{motor} = 70$ ms. The `idealgaussianobserverwithlapse`  model has two free parameters, $\\theta = \\left(\\sigma, \\lambda \\right)$. As a sanity check, compute the log-likelihood of model parameter $\\theta_\\star = \\left(\\sigma = 100, \\lambda = 0.01\\right)$ for the dataset of subject 5 (having removed the first session) and report the result in Moodle.\n",
    "- d) Separately fit the `idealgaussianobserverwithlapse` model to all the six subjects' datasets (removing the first session from all data) via maximum-likelihood estimation.  For each dataset, separately compute the AIC and BIC for the two models `gaussianmixobserverwithlapse` and `idealgaussianobserverwithlapse`. Sum the AIC and BIC values across subjects to find the sum AIC (or BIC) of the two models. Report the summed AIC (and BIC) for the two models separately in Moodle.\n",
    "- e) Which model is best, according to the metrics, and why do you think it is the case? Discuss your findings (max 300 words).\n",
    "\n",
    "*Hints*:\n",
    "- Both in parts (a) and (c), the log-likelihoods that you find should be between $-6000$ and $-5800$.\n",
    "- There is an analytical solution for the posterior (and $\\hat{s}_\\text{PM}$), but there is no analytical solution for the response distribution $p(r|s)$, so you will need to use numerical integration at least to compute the response distribution.\n",
    "- Finding the maximum-likelihood solution can be difficult as the optimization landscape is nontrivial. As a sanity check, verify that your solutions are consistent across different runs. If not, you might need to run additional optimization runs to increase the chance of finding the global optimum - possibly of the order of ten or even more.\n",
    "- Beware that running multiple optimizations for all subjects will take quite some time (easily 30-60 minutes overall, possibly more - depending on your code, your computer, and how many runs you do). Model fitting is time consuming!\n",
    "- Remember that you can ask questions in the Moodle discussion forum if you need further hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import scipy as sp\n",
    "import scipy.stats as sps\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data of Experiment 3 of [AWV12] from .csv file to a Pandas dataframe\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/lacerbi/prob-cog-mod-files/main/data/awv12_exp3.csv')\n",
    "\n",
    "# Remove unused columns\n",
    "df.drop(df.columns[[6, 7, 8]], axis=1, inplace=True)\n",
    "\n",
    "# Remove rows with NaNs\n",
    "df.dropna(axis=0, inplace=True)\n",
    "\n",
    "# Example code to take the data of subject 1, excluding the first session\n",
    "subject = 5\n",
    "s = np.array(df['Stimulus (ms)'][(df['Subject id'] == subject) & (df['Session id'] > 1)])\n",
    "r = np.array(df['Response (ms)'][(df['Subject id'] == subject) & (df['Session id'] > 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject id</th>\n",
       "      <th>Session id</th>\n",
       "      <th>Run id</th>\n",
       "      <th>Stimulus (ms)</th>\n",
       "      <th>Response (ms)</th>\n",
       "      <th>Stimulus id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>973.327049</td>\n",
       "      <td>862.947945</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>677.519900</td>\n",
       "      <td>574.920276</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>826.253049</td>\n",
       "      <td>870.995615</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>677.854859</td>\n",
       "      <td>695.055098</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>598.501198</td>\n",
       "      <td>632.981845</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Subject id  Session id  Run id  Stimulus (ms)  Response (ms)  Stimulus id\n",
       "0           1           1       1     973.327049     862.947945          6.0\n",
       "1           1           1       1     677.519900     574.920276          2.0\n",
       "2           1           1       1     826.253049     870.995615          4.0\n",
       "3           1           1       1     677.854859     695.055098          2.0\n",
       "4           1           1       1     598.501198     632.981845          1.0"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_response(s,theta):\n",
    "\n",
    "    # Unpack parameter vector theta\n",
    "    sigma = theta[0]\n",
    "    mu_prior = theta[3]\n",
    "    sigma_prior1 = theta[4]\n",
    "    sigma_prior2 = theta[5]\n",
    "    sigma_motor = 70.   \n",
    "    # Compute mean and std of the response\n",
    "    w1 = sigma_prior1**2/(sigma_prior1**2 + sigma**2)   \n",
    "    w2 = sigma_prior2**2/(sigma_prior2**2 + sigma**2)\n",
    "    mu_resp1 = (w1*s + (1-w1)*mu_prior)\n",
    "    mu_resp2 = (w2*s + (1-w2)*mu_prior)\n",
    "    \n",
    "    sigma_resp1 = np.sqrt(w1**2*sigma**2 + sigma_motor**2)\n",
    "    sigma_resp2 = np.sqrt(w2**2*sigma**2 + sigma_motor**2)\n",
    "    \n",
    "    return mu_resp1, mu_resp2, sigma_resp1, sigma_resp2\n",
    "def idealgaussianobserver_loglike(theta,s_vec,r_vec):\n",
    "\n",
    "    w_prior = theta[2]\n",
    "    sigma = theta[0]\n",
    "    lambd = theta[1]\n",
    "    mu_prior = theta[3]\n",
    "    sigma_prior1 = theta[4]\n",
    "    sigma_prior2 = theta[5]\n",
    "    mu_resp1, mu_resp2, sigma_resp1, sigma_resp2 = gaussian_response(s_vec,np.array((sigma,lambd,w_prior,mu_prior,sigma_prior1,sigma_prior2)))\n",
    "    like_vec = (1-lambd)*(w_prior * sps.norm.pdf(r_vec,mu_resp1,sigma_resp1) + (1-w_prior) * sps.norm.pdf(r_vec,mu_resp2,sigma_resp2))+ lambd / 1500\n",
    "    loglike_vec = np.log(like_vec)# Vector of log-likelihood per trials\n",
    "    loglike = np.sum(loglike_vec) # Total log-likelihood\n",
    "    return loglike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a) The log-likelihood of theta_test = [1.00e+02 1.00e-02 5.00e-01 7.87e+02 1.00e+02 1.20e+02] (dataset S5) is: -5973.238970895034\n"
     ]
    }
   ],
   "source": [
    "subject = 5\n",
    "s = np.array(df['Stimulus (ms)'][(df['Subject id'] == subject) & (df['Session id'] > 1)])\n",
    "r = np.array(df['Response (ms)'][(df['Subject id'] == subject) & (df['Session id'] > 1)])\n",
    "theta = np.array((100.,0.01,0.5,787.,100.,120.))\n",
    "loglike_test = idealgaussianobserver_loglike(theta,s,r)\n",
    "print('a) The log-likelihood of theta_test = {} (dataset S{}) is: {}'.format(\n",
    "    theta, subject, loglike_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5973.238970895034072"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float128(loglike_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 1\n",
    "s = np.array(df['Stimulus (ms)'][(df['Subject id'] == subject) & (df['Session id'] > 1)])\n",
    "r = np.array(df['Response (ms)'][(df['Subject id'] == subject) & (df['Session id'] > 1)])\n",
    "target_fun1 = lambda theta_: -idealgaussianobserver_loglike(np.array(theta_),s,r)\n",
    "subject1 = 2\n",
    "s1 = np.array(df['Stimulus (ms)'][(df['Subject id'] == subject1) & (df['Session id'] > 1)])\n",
    "r1 = np.array(df['Response (ms)'][(df['Subject id'] == subject1) & (df['Session id'] > 1)])\n",
    "target_fun2 = lambda theta_: -idealgaussianobserver_loglike(np.array(theta_),s1,r1)\n",
    "subject2 = 3\n",
    "s2 = np.array(df['Stimulus (ms)'][(df['Subject id'] == subject2) & (df['Session id'] > 1)])\n",
    "r2 = np.array(df['Response (ms)'][(df['Subject id'] == subject2) & (df['Session id'] > 1)])\n",
    "target_fun3 = lambda theta_: -idealgaussianobserver_loglike(np.array(theta_),s2,r2)\n",
    "subject3 = 4\n",
    "s3 = np.array(df['Stimulus (ms)'][(df['Subject id'] == subject3) & (df['Session id'] > 1)])\n",
    "r3 = np.array(df['Response (ms)'][(df['Subject id'] == subject3) & (df['Session id'] > 1)])\n",
    "target_fun4 = lambda theta_: -idealgaussianobserver_loglike(np.array(theta_),s3,r3)\n",
    "subject4 = 5\n",
    "s4 = np.array(df['Stimulus (ms)'][(df['Subject id'] == subject4) & (df['Session id'] > 1)])\n",
    "r4 = np.array(df['Response (ms)'][(df['Subject id'] == subject4) & (df['Session id'] > 1)])\n",
    "target_fun5 = lambda theta_: -idealgaussianobserver_loglike(np.array(theta_),s4,r4)\n",
    "subject5 = 6\n",
    "s5 = np.array(df['Stimulus (ms)'][(df['Subject id'] == subject5) & (df['Session id'] > 1)])\n",
    "r5 = np.array(df['Response (ms)'][(df['Subject id'] == subject5) & (df['Session id'] > 1)])\n",
    "target_fun6 = lambda theta_: -idealgaussianobserver_loglike(np.array(theta_),s5,r5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigma,lambd,w_prior,mu_prior,sigma_prior1,sigma_prior2\n",
    "lower_bounds = np.array([1.,0.,0.,0.,1.,1.,])\n",
    "upper_bounds = np.array([2000.,1.,1.,2000,2000,2000])\n",
    "plausible_lower_bounds = np.array([np.std(s3)*0.1,0,0,np.mean(s3)*0.1,np.std(s3)*0.1,np.std(s3)*0.1])\n",
    "plausible_upper_bounds = np.array([np.std(s3)*10,0.2,1,np.mean(s3)*10,np.std(s3)*10,np.std(s3)*10])\n",
    "def multioptimize(target_fun,lower_bounds,upper_bounds,plausible_lower_bounds,plausible_upper_bounds,num_runs=100):\n",
    "    \"\"\"Simple function for multi-start optimization.\"\"\"\n",
    "    # Run num_runs optimization runs from different starting points    \n",
    "    num_params = lower_bounds.shape[0]\n",
    "    theta_res = np.zeros((num_runs,num_params))\n",
    "    nll_res = np.zeros(num_runs)    \n",
    "    for index in range(num_runs):\n",
    "        if index == 0:\n",
    "            theta0 = 0.5*(plausible_lower_bounds + plausible_upper_bounds)\n",
    "        else:\n",
    "            theta0 = np.random.uniform(low=plausible_lower_bounds,high=plausible_upper_bounds)    \n",
    "        bounds = sp.optimize.Bounds(lower_bounds,upper_bounds,True) # Set hard bounds\n",
    "        res = sp.optimize.minimize(target_fun, theta0, method='L-BFGS-B', bounds=bounds)\n",
    "        nll_res[index] = res.fun\n",
    "        theta_res[index] = res.x\n",
    "        print('Run {}: log-likelihood {}'.format(index, -res.fun))\n",
    "        \n",
    "    # Pick the best solution\n",
    "    idx_best = np.argmin(nll_res)\n",
    "    nll_best = nll_res[idx_best]\n",
    "    theta_best = theta_res[idx_best]        \n",
    "    return nll_best,theta_best\n",
    "nll1_best,theta1_best = multioptimize(target_fun4,lower_bounds,upper_bounds,plausible_lower_bounds,plausible_upper_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_response_1(s,theta):\n",
    "    \"\"\"Compute mean and standard deviation of p(r|s; theta).\"\"\"\n",
    "    # Unpack parameter vector theta\n",
    "    # Unpack parameter vector theta\n",
    "    mu_prior = 787.5\n",
    "    sigma_prior = 128.1\n",
    "    sigma_motor = 70\n",
    "    sigma = theta[0]\n",
    "    # Compute mean and std of the response\n",
    "    w = sigma_prior**2/(sigma_prior**2 + sigma**2)\n",
    "    mu_resp = w*s + (1-w)*mu_prior\n",
    "    sigma_resp = np.sqrt(w**2*sigma**2 + sigma_motor**2)\n",
    "    return mu_resp, sigma_resp\n",
    "def idealgaussianobserver_loglike_1(theta,s_vec,r_vec):\n",
    "    sigma = theta[0]\n",
    "    lamb = theta[1]\n",
    "    mu_resp, sigma_resp = gaussian_response_1(s_vec,np.array((sigma,lamb)))\n",
    "    like_vec = (1 - lamb) * sps.norm.pdf(r_vec,mu_resp,sigma_resp) + (lamb) / 1500\n",
    "    loglike_vec = np.log(like_vec)# Vector of log-likelihood per trials\n",
    "    loglike = np.sum(loglike_vec) # Total log-likelihood\n",
    "    return loglike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a) The log-likelihood of theta_test = [1.e+02 1.e-02] (dataset S3) is: -6138.496698073355\n"
     ]
    }
   ],
   "source": [
    "subject = 3\n",
    "s = np.array(df['Stimulus (ms)'][(df['Subject id'] == subject) & (df['Session id'] > 1)])\n",
    "r = np.array(df['Response (ms)'][(df['Subject id'] == subject) & (df['Session id'] > 1)])\n",
    "theta_star = np.array((100,0.01))\n",
    "loglike_test = idealgaussianobserver_loglike_1(theta,s,r)\n",
    "print('a) The log-likelihood of theta_test = {} (dataset S{}) is: {}'.format(\n",
    "    theta_star, subject, loglike_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 1\n",
    "s = np.array(df['Stimulus (ms)'][(df['Subject id'] == subject) & (df['Session id'] > 1)])\n",
    "r = np.array(df['Response (ms)'][(df['Subject id'] == subject) & (df['Session id'] > 1)])\n",
    "target_fun1 = lambda theta_: -idealgaussianobserver_loglike_1(np.array(theta_),s,r)\n",
    "subject1 = 2\n",
    "s1 = np.array(df['Stimulus (ms)'][(df['Subject id'] == subject1) & (df['Session id'] > 1)])\n",
    "r1 = np.array(df['Response (ms)'][(df['Subject id'] == subject1) & (df['Session id'] > 1)])\n",
    "target_fun2 = lambda theta_: -idealgaussianobserver_loglike_1(np.array(theta_),s1,r1)\n",
    "subject2 = 3\n",
    "s2 = np.array(df['Stimulus (ms)'][(df['Subject id'] == subject2) & (df['Session id'] > 1)])\n",
    "r2 = np.array(df['Response (ms)'][(df['Subject id'] == subject2) & (df['Session id'] > 1)])\n",
    "target_fun3 = lambda theta_: -idealgaussianobserver_loglike_1(np.array(theta_),s2,r2)\n",
    "subject3 = 4\n",
    "s3 = np.array(df['Stimulus (ms)'][(df['Subject id'] == subject3) & (df['Session id'] > 1)])\n",
    "r3 = np.array(df['Response (ms)'][(df['Subject id'] == subject3) & (df['Session id'] > 1)])\n",
    "target_fun4 = lambda theta_: -idealgaussianobserver_loglike_1(np.array(theta_),s3,r3)\n",
    "subject4 = 5\n",
    "s4 = np.array(df['Stimulus (ms)'][(df['Subject id'] == subject4) & (df['Session id'] > 1)])\n",
    "r4 = np.array(df['Response (ms)'][(df['Subject id'] == subject4) & (df['Session id'] > 1)])\n",
    "target_fun5 = lambda theta_: -idealgaussianobserver_loglike_1(np.array(theta_),s4,r4)\n",
    "subject5 = 6\n",
    "s5 = np.array(df['Stimulus (ms)'][(df['Subject id'] == subject5) & (df['Session id'] > 1)])\n",
    "r5 = np.array(df['Response (ms)'][(df['Subject id'] == subject5) & (df['Session id'] > 1)])\n",
    "target_fun6 = lambda theta_: -idealgaussianobserver_loglike_1(np.array(theta_),s5,r5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigma,lambd,w_prior,mu_prior,sigma_prior1,sigma_prior2\n",
    "lower_bounds = np.array([1.,0.,0.,0.,1.,1.,])\n",
    "upper_bounds = np.array([2000.,1.,1.,2000,2000,2000])\n",
    "plausible_lower_bounds = np.array([np.std(s)*0.1,0,0,np.mean(s)*0.1,np.std(s)*0.1,np.std(s)*0.1])\n",
    "plausible_upper_bounds = np.array([np.std(s)*5,0.1,1,np.mean(s)*5,np.std(s)*5,np.std(s)*5])\n",
    "def multioptimize(target_fun,lower_bounds,upper_bounds,plausible_lower_bounds,plausible_upper_bounds,num_runs=100):\n",
    "    \"\"\"Simple function for multi-start optimization.\"\"\"\n",
    "    # Run num_runs optimization runs from different starting points    \n",
    "    num_params = lower_bounds.shape[0]\n",
    "    theta_res = np.zeros((num_runs,num_params))\n",
    "    nll_res = np.zeros(num_runs)    \n",
    "    for index in range(num_runs):\n",
    "        if index == 0:\n",
    "            theta0 = 0.5*(plausible_lower_bounds + plausible_upper_bounds)\n",
    "        else:\n",
    "            theta0 = np.random.uniform(low=plausible_lower_bounds,high=plausible_upper_bounds)    \n",
    "        bounds = sp.optimize.Bounds(lower_bounds,upper_bounds,True) # Set hard bounds\n",
    "        res = sp.optimize.minimize(target_fun, theta0, method='L-BFGS-B', bounds=bounds)\n",
    "        nll_res[index] = res.fun\n",
    "        theta_res[index] = res.x\n",
    "        print('Run {}: log-likelihood {}'.format(index, -res.fun))\n",
    "        \n",
    "    # Pick the best solution\n",
    "    idx_best = np.argmin(nll_res)\n",
    "    nll_best = nll_res[idx_best]\n",
    "    theta_best = theta_res[idx_best]        \n",
    "    return nll_best,theta_best\n",
    "nll1_best,theta1_best = multioptimize(target_fun6,lower_bounds,upper_bounds,plausible_lower_bounds,plausible_upper_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_log_likelihood = [-11624.338670148523,-5636.6277280675695, -6137.2636900877515,-6103.817274157488,-5913.269587083499,-5941.674021945246]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauu_mix_log_likelihood = [-11516.85261460861,-5536.184845287687,-6120.390595335808,-5986.303099562985,-5908.863719418259,-5932.807994204546]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11516.85261460861 -11624.338670148523\n",
      "-5536.184845287687 -5636.6277280675695\n",
      "-6120.390595335808 -6137.2636900877515\n",
      "-5986.303099562985 -6103.817274157488\n",
      "-5908.863719418259 -5913.269587083499\n",
      "-5932.807994204546 -5941.674021945246\n",
      "AIC GaussianMix:  82074.80573683578\n",
      "AIC IdealGaussian:  82737.98194298016\n",
      "BIC GaussianMix:  82256.01380328374\n",
      "BIC IdealGaussian:  82798.38463179614\n"
     ]
    }
   ],
   "source": [
    "# Define the number of parameters for each model\n",
    "n_params_gaussianmix = 6\n",
    "n_params_idealgaussian = 2\n",
    "\n",
    "# Initialize the sum AIC and BIC values for each model\n",
    "sum_aic_gaussianmix = 0\n",
    "sum_bic_gaussianmix = 0\n",
    "sum_aic_idealgaussian = 0\n",
    "sum_bic_idealgaussian = 0\n",
    "i = 0\n",
    "for data in [s,s1,s2,s3,s4,s5]:\n",
    "    responses = data.shape[0]\n",
    "    aic_gaussianmix = 2 * n_params_gaussianmix - 2 * gauu_mix_log_likelihood[i]\n",
    "    aic_idealgaussian = 2 * n_params_idealgaussian - 2 * ideal_log_likelihood[i]\n",
    "    bic_gaussianmix = np.log(responses) * n_params_gaussianmix - 2 * gauu_mix_log_likelihood[i]\n",
    "    bic_idealgaussian = np.log(responses) * n_params_idealgaussian - 2 * ideal_log_likelihood[i]\n",
    "    print(gauu_mix_log_likelihood[i],ideal_log_likelihood[i])\n",
    "    i = i + 1\n",
    "    sum_aic_gaussianmix += aic_gaussianmix\n",
    "    sum_aic_idealgaussian += aic_idealgaussian\n",
    "    sum_bic_gaussianmix += bic_gaussianmix\n",
    "    sum_bic_idealgaussian += bic_idealgaussian\n",
    "print('AIC GaussianMix: ', sum_aic_gaussianmix)\n",
    "print('AIC IdealGaussian: ', sum_aic_idealgaussian)\n",
    "print('BIC GaussianMix: ', sum_bic_gaussianmix)\n",
    "print('BIC IdealGaussian: ', sum_bic_idealgaussian)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "79665e5acb2b575c94fbeec7275a99a8c90a2feb0e5197b703488eab2cb26c87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
